# 第十四章：因果推断与机器学习

机器学习在预测任务上取得了巨大成功，但传统的监督学习方法主要关注相关性而非因果性。当我们需要理解干预效果、进行反事实推理或确保模型在分布偏移下的鲁棒性时，将因果推断与机器学习相结合变得至关重要。本章将探讨如何将因果思维融入现代机器学习方法，包括因果正则化、因果表示学习、因果强化学习以及算法公平性的因果视角。

## 14.1 因果正则化

### 14.1.1 动机与背景

传统机器学习模型往往过度依赖虚假相关性（spurious correlations），导致在分布偏移（distribution shift）下性能大幅下降。因果正则化通过将因果结构知识编码到学习算法中，引导模型学习真正的因果关系而非表面相关性。

考虑一个图像分类任务：识别奶牛。传统模型可能学习到"绿色草地"与"奶牛"的强相关性，因为训练数据中大部分奶牛照片都有草地背景。但这种相关性是虚假的——奶牛在沙滩上依然是奶牛。因果正则化帮助模型关注真正的因果特征。

### 14.1.2 因果不变性原则

因果不变性原则（Causal Invariance Principle）是因果正则化的核心思想：

**定义**：如果特征 $X$ 对结果 $Y$ 有因果效应，那么条件分布 $P(Y|X)$ 在不同环境下应该保持不变。

数学表述：
$$P_e(Y|X_{causal}) = P_{e'}(Y|X_{causal}), \forall e, e' \in \mathcal{E}$$

其中 $\mathcal{E}$ 是所有可能环境的集合，$X_{causal}$ 是具有因果效应的特征。

### 14.1.3 不变风险最小化（IRM）

IRM是实现因果正则化的一种重要方法：

**目标函数**：
$$\min_{\Phi, w} \sum_{e \in \mathcal{E}_{tr}} R^e(\Phi \circ w) + \lambda \cdot \text{Var}_{e \in \mathcal{E}_{tr}}[\nabla_w R^e(1 \cdot w)]$$

其中：
- $\Phi$：特征提取器
- $w$：线性分类器
- $R^e$：环境 $e$ 中的风险
- 第二项强制梯度在不同环境中保持一致

**实践中的IRM算法**：
```
输入：多环境数据 {(X^e, Y^e)}_{e∈E}
1. 初始化：特征提取器 Φ，分类器 w
2. 对每个训练轮次：
   a. 计算每个环境的损失：L^e = loss(w·Φ(X^e), Y^e)
   b. 计算梯度惩罚：penalty = Var_e[||∇_w L^e||^2]
   c. 更新参数：minimize Σ_e L^e + λ·penalty
3. 返回：学习到的模型 w·Φ
```

### 14.1.4 因果结构约束

将已知的因果图结构作为正则化约束：

**结构化方程约束**：
如果我们知道因果图 $G$，可以添加约束：
$$\mathcal{L}_{struct} = \sum_{(i,j) \notin E(G)} |\theta_{ij}|$$

其中 $\theta_{ij}$ 表示变量 $i$ 对变量 $j$ 的影响系数，$E(G)$ 是因果图的边集。

**独立性约束**：
根据d-分离准则，添加条件独立性约束：
$$\mathcal{L}_{indep} = \sum_{X \perp\!\!\!\perp Y | Z} \text{MI}(X; Y | Z)$$

其中 MI 表示条件互信息。

## 14.2 因果表示学习

### 14.2.1 概念与目标

因果表示学习旨在从高维观测数据中学习低维的因果因子（causal factors），这些因子满足：
1. **解耦性**：不同因子独立变化
2. **可解释性**：每个因子对应明确的语义
3. **可干预性**：可以独立操作每个因子

与传统表示学习的区别：
- 传统方法：学习统计上有效的压缩表示
- 因果方法：学习反映数据生成过程的因果机制

### 14.2.2 独立因果机制（ICM）原则

ICM原则假设：
$$P(X_1, ..., X_n) = \prod_{i=1}^n P(X_i | PA_i)$$

其中 $PA_i$ 是 $X_i$ 的因果父节点，且每个条件分布 $P(X_i | PA_i)$ 独立变化。

**关键性质**：
1. **模块性**：改变一个机制不影响其他机制
2. **稀疏性**：每个变量只依赖少数父节点
3. **最小变化原则**：干预通常只影响少数机制

### 14.2.3 变分自编码器与因果解耦

**因果VAE架构**：
```
观测 x → 编码器 q(z|x) → 因果潜变量 z → 因果图 G → 解码器 p(x|z)
```

损失函数：
$$\mathcal{L} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - \beta \cdot KL[q(z|x)||p(z)] + \gamma \cdot \mathcal{L}_{causal}$$

其中因果损失项 $\mathcal{L}_{causal}$ 强制潜变量遵循因果结构：
$$\mathcal{L}_{causal} = \sum_{i,j} \mathbb{1}_{(i,j) \notin G} \cdot |\text{Cov}(z_i, z_j)|$$

### 14.2.4 对比学习与因果发现

利用对比学习识别因果特征：

**因果对比损失**：
$$\mathcal{L}_{CCL} = -\log \frac{\exp(\text{sim}(z_i, z_i^+)/\tau)}{\sum_{k} \exp(\text{sim}(z_i, z_k)/\tau)}$$

其中：
- $z_i^+$：通过因果增强得到的正样本
- 因果增强：只改变非因果特征（如背景、光照）
- $\tau$：温度参数

**多环境对比学习**：
利用多环境数据识别不变特征：
```
对每个批次：
1. 从不同环境采样：x_e1, x_e2, ..., x_eK
2. 提取特征：z_e1 = f(x_e1), ...
3. 最大化环境间因果特征的一致性
4. 最小化环境间虚假特征的相似性
```

### 14.2.5 因果解耦的评估指标

**解耦度量**：
- **MIG (Mutual Information Gap)**：
  $$MIG = \frac{1}{K} \sum_{k=1}^K \frac{1}{H(v_k)} (I(z_{j^*}; v_k) - \max_{j \neq j^*} I(z_j; v_k))$$
  
- **SAP (Separated Attribute Predictability)**：
  评估每个潜变量对单一生成因子的预测能力

**因果性度量**：
- **介入鲁棒性**：模型在因果介入下的性能保持
- **反事实一致性**：生成的反事实样本与真实反事实的一致性

## 14.3 因果强化学习

### 14.3.1 强化学习中的因果问题

传统强化学习面临的因果挑战：
1. **混杂偏差**：历史策略导致的观测数据偏差
2. **信度分配问题**：如何确定哪个动作导致了奖励
3. **泛化困难**：在新环境中性能下降
4. **样本效率低**：需要大量交互才能学习

因果视角提供的解决方案：
- 利用因果模型进行反事实推理
- 基于因果图的信度分配
- 学习因果不变的策略
- 通过因果模型进行离线学习

### 14.3.2 因果模型增强的MDP

**因果MDP定义**：
扩展标准MDP为 $\mathcal{M} = (S, A, P, R, \gamma, G)$，其中：
- $G$：状态-动作-奖励的因果图
- 转移函数分解：$P(s'|s,a) = \prod_i P(s'_i | PA_i^G)$

**结构化状态表示**：
将状态分解为因果相关的组件：
$$S = S_{agent} \times S_{env} \times S_{confound}$$

其中：
- $S_{agent}$：可控状态（位置、速度）
- $S_{env}$：环境状态（障碍物、目标）
- $S_{confound}$：混杂因素（其他智能体）

### 14.3.3 反事实策略评估

**离线策略评估问题**：
给定历史数据 $\mathcal{D} = \{(s_t, a_t, r_t, s_{t+1})\}$ 由行为策略 $\pi_b$ 生成，评估目标策略 $\pi_e$ 的性能。

**反事实推理方法**：
1. **构建因果模型**：
   $$M: S \times A \rightarrow S' \times R$$

2. **反事实查询**：
   "如果在状态 $s_t$ 采取动作 $\pi_e(s_t)$ 而非 $a_t$，会得到什么奖励？"
   $$r_{CF} = R(s_t, \pi_e(s_t); U_t)$$
   其中 $U_t$ 是从观测数据推断的潜在混杂因素

3. **重要性采样修正**：
   $$V^{\pi_e} = \mathbb{E}_{\tau \sim \pi_b}\left[\sum_t \gamma^t r_t \prod_{k=0}^t \frac{\pi_e(a_k|s_k)}{\pi_b(a_k|s_k)}\right]$$

### 14.3.4 因果发现与探索

**因果好奇心驱动探索**：
定义内在奖励基于因果不确定性：
$$r_{intrinsic} = H(G | \mathcal{D}) - H(G | \mathcal{D} \cup \{(s,a,s',r)\})$$

这鼓励智能体采取能最大程度减少因果图不确定性的动作。

**主动介入学习**：
```
算法：因果主动学习
1. 初始化：因果假设集 H = {G_1, ..., G_k}
2. 对每个时间步：
   a. 计算每个动作的信息增益：
      IG(a) = H(H) - E[H(H|s',r)|s,a]
   b. 选择动作：a* = argmax_a [IG(a) + ε·Q(s,a)]
   c. 执行动作，观测结果
   d. 更新因果假设的后验概率
3. 返回：最可能的因果图 G*
```

### 14.3.5 因果模型基础规划

**世界模型与因果推理**：
利用学习到的因果模型进行规划：

1. **前向模拟**：
   $$s_{t+k} = f_{causal}(s_t, a_t, ..., a_{t+k-1})$$

2. **反事实规划**：
   在规划时考虑"如果不这样做会怎样"：
   $$Q_{CF}(s,a) = R(s,a) + \gamma \max_{a'} V(f(s,a)) - \lambda \cdot \text{Regret}(s,a)$$
   
   其中遗憾项：
   $$\text{Regret}(s,a) = \max_{a' \neq a} Q_{CF}(s,a') - Q_{CF}(s,a)$$

3. **因果层次规划**：
   - 高层：基于因果效应的子目标选择
   - 低层：基于局部动力学的动作执行

## 14.4 公平性与因果推断

### 14.4.1 算法公平性的因果视角

传统公平性定义的局限性：
- **统计平等**（Statistical Parity）：$P(\hat{Y}=1|A=0) = P(\hat{Y}=1|A=1)$
- **机会平等**（Equal Opportunity）：$P(\hat{Y}=1|Y=1,A=0) = P(\hat{Y}=1|Y=1,A=1)$

这些定义忽略了因果路径，可能导致：
1. 禁止使用合理的预测因子
2. 无法区分直接歧视和间接影响
3. 不同公平性准则之间的冲突

### 14.4.2 因果公平性框架

**因果图表示**：
```
    A (敏感属性)
    ├── 直接路径 → Y (结果)
    └── 间接路径 → M (中介变量) → Y
```

**路径特定效应**：
- **直接效应（DE）**：$A$ 对 $Y$ 的直接因果效应
  $$DE = \mathbb{E}[Y(a=1, M(a=0)) - Y(a=0, M(a=0))]$$

- **间接效应（IE）**：通过中介变量 $M$ 的效应
  $$IE = \mathbb{E}[Y(a=1, M(a=1)) - Y(a=1, M(a=0))]$$

- **总效应（TE）**：$TE = DE + IE$

### 14.4.3 反事实公平性

**定义**：如果在反事实世界中，改变个体的敏感属性不会改变决策结果，则称决策是反事实公平的。

数学表述：
$$P(\hat{Y}_{A \leftarrow a}(u) = \hat{Y}_{A \leftarrow a'}(u)) = 1, \forall u, a, a'$$

其中 $u$ 表示个体的所有外生变量。

**实现方法**：
1. **路径阻断**：阻断不公平的因果路径
   ```python
   # 伪代码
   def fair_prediction(x, sensitive_attr):
       # 1. 预测中介变量（在反事实世界中）
       m_cf = predict_mediator(x, sensitive_attr=baseline)
       # 2. 基于反事实中介变量预测
       y_pred = predict_outcome(x, m_cf)
       return y_pred
   ```

2. **公平表示学习**：
   学习与敏感属性因果独立的表示：
   $$\min_f \mathcal{L}_{task}(f(X), Y) + \lambda \cdot \mathcal{L}_{indep}(f(X), A)$$

### 14.4.4 因果解释与审计

**个体公平性解释**：
对于被拒绝的申请者，提供因果解释：
- 哪些因素导致了拒绝？
- 如果改变哪些可控因素，结果会改变？

**算法审计流程**：
```
1. 构建决策系统的因果图
2. 识别所有从敏感属性到决策的因果路径
3. 量化每条路径的因果效应
4. 判断哪些路径是合理的，哪些构成歧视
5. 评估整体公平性水平
```

### 14.4.5 公平性与效用的权衡

**多目标优化框架**：
$$\min_{f} \mathcal{L}_{utility}(f) + \sum_{i} \lambda_i \cdot \mathcal{L}_{fairness}^i(f)$$

其中不同的公平性约束：
- $\mathcal{L}_{fairness}^{DE}$：最小化直接歧视
- $\mathcal{L}_{fairness}^{IE}$：控制间接效应
- $\mathcal{L}_{fairness}^{CF}$：反事实公平性

**帕累托前沿分析**：
找出效用和公平性之间的帕累托最优解集，让决策者根据具体场景选择合适的权衡点。
